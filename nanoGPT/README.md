# nanoGPT

nanoGPT - Implementation of nanoGPT from Andrej Karpathy. The autoregressive decoder transformer is a lightweight generative pre-trained transformer (GPT) model that uses character-level tokenisation to predict the next character in a sequence of generated text.

main.py - Python script file to train and log the performance of the training with Hydra.

model.py - nanoGPT or the Bigram Language transformer model from scratch in Python with PyTorch.

The tiny_shakespere dataset is available from HuggingFace at: {\url{https://huggingface.co/datasets/karpathy/tiny_shakespeare}}

<br>

## HugginFace Dataset BibTex Citation

    @misc{
    author={Karpathy, Andrej},
    title={char-rnn},
    year={2015},
    howpublished={\url{https://github.com/karpathy/char-rnn}}
    }

<br>

## nanoGPT GitHub Repository BibTex Citation

    @misc{Karpathy_2023,  
    author = {Karpathy, Andrej},
    title = {{nanogpt-lecture}},
    howpublished = {GitHub},
    month = {Jan. 17},   
    year = {2023},
    url = {{https://github.com/karpathy/ng-video-lecture}}
    }
